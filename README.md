# LLM Response Evaluation Framework

This repository implements a modular and production-oriented framework for evaluating responses generated by Large Language Models (LLMs).  
The pipeline measures:

- Relevance between the model response and reference context  
- Hallucination and factual consistency using NLI  
- Latency of execution  
- Token-based cost estimation  

The goal is to provide a clean, extensible foundation for LLM evaluation at scale.

---

## Features

### 1. Relevance Scoring  
Uses SentenceTransformer embeddings (`all-MiniLM-L6-v2`) to compute semantic similarity between the model’s response and the provided context.

### 2. Hallucination Detection  
Employs an MNLI model (`roberta-large-mnli`) to classify each sentence in the response as:
- Supported  
- Contradicted  
- Unsupported  

### 3. Latency Measurement  
A simple timing utility for measuring execution time and profiling system performance.

### 4. Token Cost Estimation  
Estimates cost based on input/output token counts and configurable pricing.

---

## Project Structure

llm-evaluation-pipeline/
│
├── evaluator/
│ ├── relevance.py
│ ├── hallucination.py
│ ├── latency.py
│ ├── cost.py
│ └── pipeline.py
│
├── examples/
│ ├── conversation.json
│ └── context.json
│
├── tests/
│ └── test_relevance.py
│
├── docs/
│ └── architecture.md
│
├── .github/
│ └── workflows/
│ └── ci.yml
│
├── main.py
├── benchmark.py
├── Dockerfile
├── requirements.txt
└── README.md

yaml
Copy code

This structure follows standard Python practices and is suitable for CI integration, modular development, and testing.

---

## Installation

### 1. Create and activate a virtual environment

Windows (PowerShell):
```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
Linux / macOS / Git Bash:

bash
Copy code
python -m venv .venv
source .venv/bin/activate
2. Install dependencies
bash
Copy code
pip install -r requirements.txt
Running an Evaluation
bash
Copy code
python main.py
Expected output structure:

json
Copy code
{
  "relevance": 0.82,
  "hallucination": { ... },
  "latency_ms": 0.12,
  "estimated_cost": 0.00003
}
Running Tests
bash
Copy code
pytest -q
Tests are automatically executed on every push through GitHub Actions defined in ci.yml.

Docker Usage
Build the image:

bash
Copy code
docker build -t llm-eval .
Run the container:

bash
Copy code
docker run --rm -it llm-eval
Design Rationale
Why this solution?
Embedding-based relevance scoring
Provides more accurate semantic matching than keyphrase or TF-IDF based approaches.

NLI for hallucination detection
MNLI models evaluate logical relationships between context and output, making them more reliable for detecting unsupported claims.

Modular architecture
Each evaluation dimension (relevance, hallucination, latency, cost) is isolated, enabling independent improvement and scalability.

JSON input format
Mirrors real-world conversation logs and retrieval outputs, making the pipeline straightforward to integrate into production systems.

Lightweight dependencies
Only essential NLP and utility libraries are used, optimizing installation and runtime performance.

Scaling Strategy for Millions of Daily Evaluations
1. Embedding Caching
Context embeddings rarely change; caching them in Redis or a vector database removes repeated computation and reduces latency.

2. Batching NLI Inference
Running NLI per sentence is expensive. Batching 32–128 sentences per inference call provides significant GPU and CPU efficiency.

3. ONNX Runtime and Quantization
Converting MNLI and embedding models to ONNX enables faster inference. FP16 or INT8 quantization reduces memory and improves speed.

4. Asynchronous Processing
Using async workers (Celery, Ray, FastAPI workers) allows parallel evaluation and smooth distribution of load.

5. Horizontal Scaling
Multiple evaluation workers can run behind a load balancer to support heavy traffic.

6. Workload Separation
CPU handles embedding models

GPU handles NLI inference
This reduces cost while maximizing throughput.

7. Optional Model Distillation
If required, smaller distilled NLI or embedding models can be used to reduce latency further.

Documentation
A detailed overview of the system architecture is available in:

bash
Copy code
docs/architecture.md
This includes design decisions, data flow, evaluation logic, and scaling considerations.

Benchmarking
Measure evaluation runtime:

bash
Copy code
python benchmark.py
License
This project is released under the MIT License.

Author
Meet Jadhav
GitHub: https://github.com/acrocantosauras
