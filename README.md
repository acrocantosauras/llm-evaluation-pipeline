# LLM Response Evaluation Framework

A clean, modular, and production-oriented framework for evaluating responses generated by Large Language Models (LLMs).  
This framework measures **relevance**, **factual consistency**, **latency**, and **token-based cost estimation**.

It is designed to be simple enough for experimentation, yet structured enough to scale for real-world LLM evaluation pipelines.

---

## üöÄ Features

### **1. Relevance Scoring**
Uses semantic similarity (sentence-transformers) to measure how aligned a model‚Äôs response is with the provided reference/context.

### **2. Hallucination Detection**
Per-sentence factual validation using an NLI (MNLI) model, classifying each sentence as:
- **Supported**
- **Contradicted**
- **Unsupported**

### **3. Latency Measurement**
Lightweight timing utility to measure model inference latency.

### **4. Token Cost Estimation**
Cost estimation based on input/output token counts and configurable pricing.

---

## üìÅ Project Structure


